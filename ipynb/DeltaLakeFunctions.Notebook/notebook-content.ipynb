{
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "%run /EnvSettings"
            ],
            "outputs": [],
            "execution_count": 21,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:36.4785618Z\",\"execution_finish_time\":\"2025-02-12T13:29:36.4788471Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "1e4915f6-fc71-43ae-a381-d2200decf903"
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Spark session configuration\n",
                "This cell sets Spark session settings to enable _Verti-Parquet_ and _Optimize on Write_."
            ],
            "metadata": {},
            "id": "f589227f-9219-429e-8ced-a7186644762d"
        },
        {
            "cell_type": "code",
            "source": [
                "from delta.tables import *\n",
                "from pyspark.sql.functions import *\n",
                "import json\n",
                "\n",
                "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\n",
                "spark.conf.set('spark.ms.autotune.queryTuning.enabled', 'true')\n",
                "\n",
                "#Low shuffle for untouched rows during MERGE\n",
                "spark.conf.set(\"spark.microsoft.delta.merge.lowShuffle.enabled\", \"true\")"
            ],
            "outputs": [],
            "execution_count": 22,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:36.643277Z\",\"execution_finish_time\":\"2025-02-12T13:29:36.9445053Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "9905195d-a4be-4bbf-8913-1f0ffad619fc"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# getAbfsPath()\n",
                "Gets the Azure Blob File System (ABFS) path of the OneLake medallion layer as URI abfss://workspaceId@onelake.dfs.fabric.microsoft.com/lakehouseID\n"
            ],
            "metadata": {},
            "id": "309ded95-96e9-4b7a-a692-ddeecd690376"
        },
        {
            "cell_type": "code",
            "source": [
                "def getAbfsPath(medallionLayer):\n",
                "    # ##########################################################################################################################  \n",
                "    # Function: getAbfsPath\n",
                "    # Gets the Azure Blob File System (ABFS) path of the OneLake medallion layer \n",
                "    # as URI abfss://workspaceId@onelake.dfs.fabric.microsoft.com/lakehouseID\n",
                "    # \n",
                "    # Parameters:\n",
                "    # medallionLayer =  Medallion layer of data platform. Valid values are bronze, silver or gold.\n",
                "    # \n",
                "    # Returns:\n",
                "    # The ABFS URI as string\n",
                "\n",
                "    validMedallionLayer = [\"bronze\",\"silver\",\"gold\"]\n",
                "    assert medallionLayer in validMedallionLayer, \"Invalid medallion layer. Valid values are bronze, silver or gold\"\n",
                "\n",
                "    AbfsPath =None\n",
                "    workspaceId = None\n",
                "    lhName = None\n",
                "\n",
                "    match medallionLayer:\n",
                "        case \"bronze\":\n",
                "            workspaceId = bronzeWorkspaceId\n",
                "            lhName = bronzeLakehouseName\n",
                "        case \"silver\":\n",
                "            workspaceId = silverWorkspaceId\n",
                "            lhName = silverLakehouseName\n",
                "        case \"gold\":\n",
                "            workspaceId = goldWorkspaceId\n",
                "            lhName = goldLakehouseName\n",
                "\n",
                "    lh= notebookutils.lakehouse.getWithProperties(lhName,workspaceId)\n",
                "    abfsPath = lh.properties[\"abfsPath\"]\n",
                "\n",
                "    return abfsPath"
            ],
            "outputs": [],
            "execution_count": 23,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:37.0906296Z\",\"execution_finish_time\":\"2025-02-12T13:29:37.3544397Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "9bda5d1b-05ce-4ba8-938d-7058e86212d0"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# readFile()\n",
                "Reads a data file from Lakehouse and returns as Spark dataframe\n"
            ],
            "metadata": {},
            "id": "926bd2dd-3a40-4d5b-bf18-226fdb2c0bd0"
        },
        {
            "cell_type": "code",
            "source": [
                "def readFile(medallionLayer,container, folder, file, colSeparator=None, headerFlag=None):\n",
                "  # ##########################################################################################################################  \n",
                "  # Function: readFile\n",
                "  # Reads a data file from Lakehouse and returns as spark dataframe\n",
                "  # \n",
                "  # Parameters:\n",
                "  # medallionLayer =  Medallion layer of data platform. Valid values are bronze, silver or gold.\n",
                "  # container = Container of Lakehouse. Default value 'Files'\n",
                "  # folder = Folder within container where data file resides. E.g 'raw-bronze/wwi/Sales/Orders/2013-01'\n",
                "  # file = File name of data file including and file extension. E.g 'Sales_Orders_2013-01-01_000000.parquet'\n",
                "  # colSeparator = Column separator for text files. Default value None\n",
                "  # headerFlag = boolean flag to indicate whether the text file has a header or not. Default value None\n",
                "  # \n",
                "  # Returns:\n",
                "  # A dataframe of the data file\n",
                "  # ##########################################################################################################################    \n",
                "    validMedallionLayer = [\"bronze\",\"silver\",\"gold\"]\n",
                "    assert medallionLayer in validMedallionLayer, \"Invalid medallion layer. Valid values are bronze, silver or gold\"\n",
                "    assert container is not None, \"container not provided\"   \n",
                "    assert folder is not None, \"folder not provided\"\n",
                "    assert file is not None, \"file not provided\"\n",
                "\n",
                "    abfsPath = getAbfsPath(medallionLayer)\n",
                "    relativePath =   container + '/' + folder +'/' + file\n",
                "    filePath = abfsPath + '/' + relativePath\n",
                "\n",
                "    if \".csv\" in file or \".txt\" in file:\n",
                "        df = spark.read.csv(path=filePath, sep=colSeparator, header=headerFlag, inferSchema=\"true\")\n",
                "    elif \".parquet\" in file:\n",
                "        df = spark.read.parquet(filePath)\n",
                "    elif \".json\" in file:\n",
                "        df = spark.read.json(filePath, multiLine= True)\n",
                "    elif \".orc\" in file:\n",
                "        df = spark.read.orc(filePath)\n",
                "    else:\n",
                "        df = spark.read.format(\"csv\").load(filePath)\n",
                "  \n",
                "    df =df.dropDuplicates()\n",
                "    return df"
            ],
            "outputs": [],
            "execution_count": 24,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:37.5304822Z\",\"execution_finish_time\":\"2025-02-12T13:29:37.8005835Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "770a1316-3980-47de-af4a-abfc2f63561c"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# readMedallionLHTable()\n",
                "Retrieves a Lakehouse Table from the medallion layers, allowing for table filtering and specific column selection"
            ],
            "metadata": {},
            "id": "9eca830f-3a83-48a8-857e-ee758f8f86f2"
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "def readMedallionLHTable(medallionLayer,tableRelativePath, filterCond=None, colList=None):\n",
                "  # ##########################################################################################################################  \n",
                "  # Function: readMedallionLHTable\n",
                "  # Retrieves a Lakehouse Table from the medallion layers, allowing for table filtering and specific column selection\n",
                "  # \n",
                "  # Parameters:\n",
                "  # medallionLayer =  Medallion layer of data platform. Valid values are bronze, silver or gold.\n",
                "  # tableRelativePath = Relative path of the LH table in format Tables/Schema/TableName\n",
                "  # filterCond = A valid filter condition for the table, passed as string. E.g \"ColorName == 'Salmon'\". Default value is None. \n",
                "  #              If filterCond is None, the full table will be returned.\n",
                "  # colList = Columns to be selected, passed as list. E.g. [\"ColorID\",\"ColorName\"]. Default value is None.\n",
                "  #           If colList is None, all columns in the table will be returned.\n",
                "  # \n",
                "  # Returns:\n",
                "  # A dataframe containing the Lakehouse table.\n",
                "  # ##########################################################################################################################    \n",
                "    validMedallionLayer = [\"bronze\",\"silver\",\"gold\"]\n",
                "    assert medallionLayer in validMedallionLayer, \"Invalid medallion layer. Valid values are bronze, silver or gold\"\n",
                "    abfsPath = getAbfsPath(medallionLayer)\n",
                "    tablePath = abfsPath + '/' + tableRelativePath\n",
                "\n",
                "    # check if table exists\n",
                "    table = DeltaTable.forPath(spark,tablePath)\n",
                "    assert table is not None, \"Lakehouse table does not exist\"\n",
                "\n",
                "    df = spark.read.format(\"delta\").load(tablePath)\n",
                "    \n",
                "    # Apply filter condition\n",
                "    if filterCond is not None:\n",
                "        df = df.filter(filterCond)\n",
                "\n",
                "    # Select columns\n",
                "    if colList is not None:\n",
                "        df = df.select(colList)\n",
                "\n",
                "    df =df.dropDuplicates()\n",
                "\n",
                "    return df"
            ],
            "outputs": [],
            "execution_count": 25,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:37.9471894Z\",\"execution_finish_time\":\"2025-02-12T13:29:38.2667513Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "91ee3e7c-a330-494d-8f97-d09551e6b158"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# readLHTable()\n",
                "Retrieves a any Lakehouse Table from OneLake, allowing for table filtering and specific column selection. Use this function to read from any Lakehouse in Onelake outside data platform for e.g. a mirrored database, Fabric SQL etc"
            ],
            "metadata": {},
            "id": "fa51f1c2-596c-4906-972b-6be62eb3cdde"
        },
        {
            "cell_type": "code",
            "source": [
                "\n",
                "def readLHTable(LakehouseName,tableRelativePath,WorkspaceID=None, filterCond=None, colList=None):\n",
                "  # ##########################################################################################################################  \n",
                "  # Function: readLHTable\n",
                "  # Retrieves a any Lakehouse Table from OneLake, allowing for table filtering and specific column selection\n",
                "  # Use this function to read from any Lakehouse in Onelake outside data platform for e.g a mirrored database, Fabric SQL etc\n",
                "  #  \n",
                "  # Parameters:\n",
                "  # LakehouseName =  Name of lakehouse where table is located\n",
                "  # tableRelativePath = Relative path of the table in format Tables/Schema/TableName\n",
                "  # WorkspaceID = ID of Fabric Workspace where lakehouse is located. Default is None\n",
                "  #               If WorkspaceID is None, the default Lakhouse attached to the notebook will be used.  \n",
                " \n",
                "  # filterCond = A valid filter condition for the table, passed as string. E.g \"ColorName == 'Salmon'\". Default value is None. \n",
                "  #              If filterCond is None, the full table will be returned.\n",
                "  # colList = Columns to be selected, passed as list. E.g. [\"ColorID\",\"ColorName\"]. Default value is None.\n",
                "  #           If colList is None, all columns in the table will be returned.\n",
                "  # \n",
                "  # Returns:\n",
                "  # A dataframe containing the Lakehouse table.\n",
                "  # ##########################################################################################################################    \n",
                "    lh = notebookutils.lakehouse.getWithProperties(LakehouseName,WorkspaceID)\n",
                "    abfsPath = lh.properties[\"abfsPath\"]\n",
                "    tablePath = abfsPath + '/' + tableRelativePath\n",
                "\n",
                "    # check if table exists\n",
                "    table = DeltaTable.forPath(spark,tablePath)\n",
                "    assert table is not None, \"Lakehouse table does not exist\"\n",
                "\n",
                "    df = spark.read.format(\"delta\").load(tablePath)\n",
                "    \n",
                "    # Apply filter condition\n",
                "    if filterCond is not None:\n",
                "        df = df.filter(filterCond)\n",
                "\n",
                "    # Select columns\n",
                "    if colList is not None:\n",
                "        df = df.select(colList)\n",
                "\n",
                "    df =df.dropDuplicates()\n",
                "\n",
                "    return df"
            ],
            "outputs": [],
            "execution_count": 26,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:38.4399576Z\",\"execution_finish_time\":\"2025-02-12T13:29:38.7221139Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "7885be98-5299-4a96-86d3-a45088ae7e8a"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# insertDelta()\n",
                "Inserts a dataframe to delta lake table. Creates a table with the schema of dataframe if the table doesn't already exist"
            ],
            "metadata": {},
            "id": "4d01a648-66ba-4ab6-ba18-af8efedc7b96"
        },
        {
            "cell_type": "code",
            "source": [
                "def insertDelta (df, tableName, writeMode=\"append\"):\n",
                "  # ##########################################################################################################################  \n",
                "  # Function: insertDelta\n",
                "  # Inserts a dataframe to delta lake table. Creates a table with the schema of dataframe if the table doesn't already exist\n",
                "  # \n",
                "  # Parameters:\n",
                "  # df = Input dataframe\n",
                "  # tableName = Target tableName in current lakehouse\n",
                "  # mode = write mode. Valid values are \"append\", \"overwrite\". Default value \"append\"\n",
                "  # \n",
                "  # Returns:\n",
                "  # Json containing operational metrics.This is useful to get information like number of records inserted/updated.\n",
                "  # E.g payload\n",
                "  # {'numOutputRows': '4432', 'numOutputBytes': '87157', 'numFiles': '1'}\n",
                "  # ##########################################################################################################################  \n",
                "    validMode =[ \"append\", \"overwrite\"]\n",
                "    assert writeMode in validMode, \"Invalid mode specified\"\n",
                "\n",
                "    # Creating a delta table with schema name not supported at the time of writing this code, so replacing schema name with \"_\". To be commented out when this\n",
                "    #feature is available\n",
                "    tableName = tableName.replace(\".\",\"_\")\n",
                "    \n",
                "    #Get delta table reference\n",
                "    DeltaTable.createIfNotExists(spark).tableName(tableName).addColumns(df.schema).execute()\n",
                "    deltaTable = DeltaTable.forName(spark,tableName)\n",
                "    tableName =deltaTable.detail()\n",
                "    tableName =deltaTable.detail().select(\"location\").first()[0].split(\"/\")[-1] #get last \"/\" substring from location attribute of delta table\n",
                "    assert tableName is not None, \"Delta table does not exist\"\n",
                "    \n",
                "    df.write.format(\"delta\").mode(writeMode).saveAsTable(tableName)\n",
                "    \n",
                "    stats = deltaTable.history(1).select(\"OperationMetrics\").first()[0]\n",
                "    print(stats)\n",
                "\n",
                "    return stats"
            ],
            "outputs": [],
            "execution_count": 27,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:38.8824344Z\",\"execution_finish_time\":\"2025-02-12T13:29:39.1670613Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "1d018a81-ca36-4a2d-8ac9-5960247f4d65"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# upsertDelta()\n",
                "Upserts a dataframe to delta lake table. Inserts record if they don't exist, updates existing if the version is newer than existing"
            ],
            "metadata": {},
            "id": "9266671d-1f26-44f1-b451-28ee8a4ee537"
        },
        {
            "cell_type": "code",
            "source": [
                "def upsertDelta(df,tableName,keyColumns,watermarkColumn=None):\n",
                "  # ##########################################################################################################################################   \n",
                "  # Function: upsertDelta\n",
                "  # Upserts a dataframe to delta lake table. Inserts record if they don't exist, updates existing if the version is newer than existing\n",
                "  # \n",
                "  # Parameters:\n",
                "  # df = Input dataframe\n",
                "  # tableName = Target tableName in current lakehouse\n",
                "  # mode = write mode. Valid values are \"append\", \"overwrite\". Default value \"append\"\n",
                "  # \n",
                "  # Returns:\n",
                "  # Json containing operational metrics.This is useful to get information like number of records inserted/updated.\n",
                "  # E.g payload\n",
                "  #     {'numOutputRows': '4432', 'numTargetRowsInserted': '0', 'numTargetFilesAdded': '1', \n",
                "  #     'numTargetFilesRemoved': '1', 'executionTimeMs': '2898', 'unmodifiedRewriteTimeMs': '606',\n",
                "  #      'numTargetRowsCopied': '0', 'rewriteTimeMs': '921', 'numTargetRowsUpdated': '4432', 'numTargetRowsDeleted': '0',\n",
                "  #      'scanTimeMs': '1615', 'numSourceRows': '4432', 'numTargetChangeFilesAdded': '0'}\n",
                "  # ##########################################################################################################################################   \n",
                "\n",
                "    # Creating a delta table with schema name not supported at the time of writing this code, so replacing schema name with \"_\". To be commented out when this\n",
                "    #feature is available\n",
                "    tableName = tableName.replace(\".\",\"_\")\n",
                "\n",
                "    #Get target table reference\n",
                "    DeltaTable.createIfNotExists(spark).tableName(tableName).addColumns(df.schema).execute()\n",
                "    target = DeltaTable.forName(spark,tableName)\n",
                "    assert target is not None, \"Target delta lake table does not exist\"\n",
                "\n",
                "    keyColumnsList = keyColumns.split(\"|\")\n",
                "   \n",
                "    #Merge Condition\n",
                "    joinCond=\"\"\n",
                "    for keyCol in keyColumnsList:\n",
                "        joinCond = joinCond + \"target.\" + keyCol + \" = source.\" + keyCol +\" and \"\n",
                "\n",
                "    remove = \"and\"\n",
                "    reverse_remove = remove[::-1]\n",
                "    joinCond = joinCond[::-1].replace(reverse_remove,\"\",1)[::-1]\n",
                "\n",
                "    if watermarkColumn is not None:\n",
                "        joinCond = joinCond + \" and target.\" + watermarkColumn + \" <= source.\" + watermarkColumn\n",
                "    \n",
                "    # Column mappings for insert and update\n",
                "    mapCols =\"\"\n",
                "    for col in df.columns:\n",
                "        mapCols = mapCols + '\"' + col + '\":' + ' \"' + 'source.' + col + '\" ,'\n",
                "\n",
                "    remove = \",\"\n",
                "    reverse_remove = remove[::-1]\n",
                "    mapCols = mapCols[::-1].replace(reverse_remove,\"\",1)[::-1]\n",
                "\n",
                "    #Convert Insert and Update Expression in to Dict\n",
                "    updateStatement = json.loads(\"{\"+mapCols+\"}\")\n",
                "    insertStatement = json.loads(\"{\"+mapCols+\"}\")\n",
                "\n",
                "    (target.alias(\"target\")\n",
                "            .merge( df.alias('source'), joinCond)\n",
                "            .whenMatchedUpdate(set = updateStatement)\n",
                "            .whenNotMatchedInsert(values = insertStatement)\n",
                "            .execute()\n",
                "     )   \n",
                "\n",
                "    # print(updateStatement)\n",
                "    stats = target.history(1).select(\"OperationMetrics\").first()[0]\n",
                "    print(stats)\n",
                "\n",
                "    return stats"
            ],
            "outputs": [],
            "execution_count": 28,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:39.3514022Z\",\"execution_finish_time\":\"2025-02-12T13:29:39.6212126Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "235c362d-717c-4fbe-97af-e0bd960265c5"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# getHighWaterMark()\n",
                "Retrieves High Watermark from a Lakehouse table for a given date range"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "f539d1da-66a2-462a-b4ad-11cc9ebf0cae"
        },
        {
            "cell_type": "code",
            "source": [
                "def getHighWaterMark(lakehouseName,tableRelativePath,watermarkColName, fromRange, toRange, workspaceID=None):\n",
                "  # ##########################################################################################################################  \n",
                "  # Function: getHighWaterMark\n",
                "  # Retrieves High Watermark from a Lakehouse table for a given date range\n",
                "  #  \n",
                "  # Parameters:\n",
                "  # lakehouseName =  Name of lakehouse where table is located\n",
                "  # tableRelativePath = Relative path of the table in format Tables/Schema/TableName\n",
                "  # watermarkColName - Name of column used for watermark\n",
                "  # fromRange - datetime of lower range of data in watermark column\n",
                "  # toRange - datetime of upper range of data in watermark column\n",
                "  # WorkspaceID = ID of Fabric Workspace where lakehouse is located. Default is None\n",
                "  #               If WorkspaceID is None, the default Lakhouse attached to the notebook will be used.  \n",
                " \n",
                "  # \n",
                "  # Returns:\n",
                "  # A max value of the high watermark column for the datetime range\n",
                "  # ##########################################################################################################################     \n",
                "    assert watermarkColName is not None,\"Watermark column name not provided\"\n",
                "    assert fromRange is not None,\"fromRange datetime not provided\"\n",
                "    assert toRange is not None,\"toRange datetime not provided\"\n",
                "\n",
                "    filterCond = watermarkColName +\">\" + \"'\" + fromRange +\"'\" + \" and \" + watermarkColName + \"<=\" + \"'\" + toRange + \"'\"\n",
                "    df = readLHTable(lakehouseName,tableRelativePath,workspaceID,filterCond,watermarkColName)\n",
                "    hwm = df.agg({watermarkColName: \"max\"}).collect()[0][0]\n",
                "\n",
                "    return hwm"
            ],
            "outputs": [],
            "execution_count": 29,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:39.7898257Z\",\"execution_finish_time\":\"2025-02-12T13:29:40.0744795Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "a7fea110-59fa-409a-b49d-bd8190a3ff07"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# optimizeDelta()\n",
                "Compacts small files and removed unused files beyond default retention period for a Delta Table"
            ],
            "metadata": {},
            "id": "0cc55498-accc-4c75-8923-30c4bcfc2fab"
        },
        {
            "cell_type": "code",
            "source": [
                "def optimizeDelta(tableName):\n",
                "  # ##########################################################################################################################################   \n",
                "  # Function: optimizeDelta\n",
                "  # Function that implements the compaction of small files for Delta Tables https://docs.delta.io/latest/optimizations-oss.html#language-python\n",
                "  # Also removes unused files beyond default retention period\n",
                "  # \n",
                "  # Parameters:\n",
                "  # tableName = Delta lake tableName in current lakehouse\n",
                "  # \n",
                "  # Returns:\n",
                "  # None\n",
                "  # ##########################################################################################################################################   \n",
                "    # Creating a delta table with schema name not supported at the time of writing this code, so replacing schema name with \"_\". To be commented out when this\n",
                "    #feature is available\n",
                "    tableName = tableName.replace(\".\",\"_\")\n",
                "\n",
                "    deltaTable = DeltaTable.forName(spark,tableName)\n",
                "    assert deltaTable is not None, \"Delta lake table does not exist\"\n",
                "    deltaTable.optimize().executeCompaction()\n",
                "    deltaTable.vacuum()\n",
                "    return"
            ],
            "outputs": [],
            "execution_count": 30,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Benny Austin\":{\"session_start_time\":null,\"execution_start_time\":\"2025-02-12T13:29:40.223704Z\",\"execution_finish_time\":\"2025-02-12T13:29:40.4920723Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "id": "5163a105-d650-429f-b4e6-417495b57b85"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "language": "Python",
            "display_name": "Synapse PySpark"
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": null,
                "default_lakehouse_name": "",
                "default_lakehouse_workspace_id": ""
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}