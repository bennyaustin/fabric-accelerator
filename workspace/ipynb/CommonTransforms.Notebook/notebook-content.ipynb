{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# What is CommonTransforms and how to use them in your notebooks ?  \n",
                "CommonTransforms is a Python class that uses PySpark libraries to apply common transformations to a Spark dataframe. https://github.com/bennyaustin/pyspark-utils/blob/main/CommonTransforms/README.md"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "13fd482e-b7bb-46cc-b408-2d80f9931c8c"
        },
        {
            "cell_type": "markdown",
            "source": [
                "# CommonTransforms Class"
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "id": "9eb1a0ea-9245-4dea-96a0-0df395b7954a"
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.functions import trim,when,isnull,lit,col,from_utc_timestamp,to_utc_timestamp,concat_ws,sha1,length,substring,lit,concat,date_add,expr,year,datediff\n",
                "from pyspark.sql import functions as F \n",
                "import datetime"
            ],
            "outputs": [],
            "execution_count": 1,
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "fdf938f6-069d-4f80-a656-a2bdeb5c8899"
        },
        {
            "cell_type": "code",
            "source": [
                "class CommonTransforms:\n",
                "  inputDf=None\n",
                "  inputSchema=None\n",
                "  inputColums=None\n",
                "  \n",
                "#   Constructor\n",
                "  def __init__(self, input):\n",
                "    self.inputDf=input\n",
                "    self.inputSchema=self.inputDf.schema\n",
                "    self.inputColumns=self.inputDf.schema.fieldNames()\n",
                "    \n",
                "#  Remove Leading and Trailing Spaces \n",
                "  def trim(self):\n",
                "    stringCol= (col for col in self.inputSchema if str(col.dataType)==\"StringType\")\n",
                "    for col in stringCol:\n",
                "        self.inputDf = self.inputDf.withColumn(col.name,trim(col.name))\n",
                "    return self.inputDf\n",
                "  \n",
                "#   Replace Null values with Default values based on datatypes\n",
                "  def replaceNull(self,value, subset=None):\n",
                "    isDate=False\n",
                "    isTimestamp =False\n",
                "    \n",
                "    try:\n",
                "      if isinstance(value, str):\n",
                "        date_obj = datetime.datetime.strptime(value, \"%Y-%m-%d\") #YYYY-MM-DD format e.g \"2020-10-01\"\n",
                "        isDate= True\n",
                "    except ValueError:\n",
                "      isDate=False\n",
                "      \n",
                "    try:\n",
                "      if isinstance(value, str):\n",
                "        date_obj = datetime.datetime.strptime(value, \"%Y-%m-%dT%H:%M:%S\") #YYYY-MM-DDThh:mm:ss format e.g \"2020-10-01T19:50:06\"\n",
                "        isTimestamp= True\n",
                "    except ValueError:\n",
                "      isTimestamp=False\n",
                "      \n",
                "    if isDate and subset is not None:\n",
                "      dateCol = (x for x in self.inputSchema if str(x.dataType)==\"DateType\" and x.nullable==True and x.name in subset)\n",
                "      for x in dateCol:\n",
                "        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\n",
                "    elif isDate and subset is None:\n",
                "      dateCol = (x for x in self.inputSchema if str(x.dataType)==\"DateType\" and x.nullable==True)\n",
                "      for x in dateCol:\n",
                "        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\n",
                "    elif isTimestamp and subset is not None:\n",
                "      tsCol = (x for x in self.inputSchema if str(x.dataType)==\"TimestampType\" and x.nullable==True and x.name in subset)\n",
                "      for x in tsCol:\n",
                "        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))\n",
                "    elif isTimestamp and subset is None:\n",
                "      tsCol = (x for x in self.inputSchema if str(x.dataType)==\"TimestampType\" and x.nullable==True)\n",
                "      for x in tsCol:\n",
                "        self.inputDf = self.inputDf.withColumn(x.name, when(isnull(col(x.name)),lit(value)).otherwise(col(x.name)))        \n",
                "    else:\n",
                "      self.inputDf = self.inputDf.fillna(value,subset)\n",
                "      \n",
                "    return self.inputDf\n",
                "\n",
                "#  Remove duplicates\n",
                "  def deDuplicate(self, subset=None):\n",
                "    self.inputDf = self.inputDf.dropDuplicates(subset)\n",
                "    return self.inputDf\n",
                "  \n",
                "#   Convert UTC timestamp to local\n",
                "  def utc_to_local(self,localTimeZone,subset=None):\n",
                "    if subset is not None:\n",
                "      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\n",
                "    else:\n",
                "      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\n",
                "      \n",
                "    for x in tsCol:\n",
                "      self.inputDf = self.inputDf.withColumn(x.name,from_utc_timestamp(col(x.name),localTimeZone))\n",
                "    return self.inputDf\n",
                "\n",
                "#   Convert timestamp in local timezone to UTC\n",
                "  def local_to_utc(self,localTimeZone,subset=None):\n",
                "    if subset is not None:\n",
                "      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\n",
                "    else:\n",
                "      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\n",
                "      \n",
                "    for x in tsCol:\n",
                "      self.inputDf = self.inputDf.withColumn(x.name,to_utc_timestamp(col(x.name),localTimeZone))\n",
                "    return self.inputDf\n",
                "  \n",
                "#   Change Timezone\n",
                "  def changeTimezone(self,fromTimezone,toTimezone,subset=None):\n",
                "    if subset is not None:\n",
                "      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\" and x.name in subset)\n",
                "    else:\n",
                "      tsCol = (x for x in  self.inputSchema if str(x.dataType)==\"TimestampType\")\n",
                "    \n",
                "    for x in tsCol:\n",
                "      self.inputDf = self.inputDf.withColumn(x.name,to_utc_timestamp(col(x.name),fromTimezone))\n",
                "      self.inputDf = self.inputDf.withColumn(x.name,from_utc_timestamp(col(x.name),toTimezone))\n",
                "    return self.inputDf\n",
                "\n",
                "#   Drop System/Non-Business Columns\n",
                "  def dropSysColumns(self,columns):\n",
                "    self.inputDf = self.inputDf.drop(columns)\n",
                "    return self.inputDf \n",
                "  \n",
                "#  Create Checksum Column \n",
                "  def addChecksumCol(self,colName):\n",
                "    self.inputDf = self.inputDf.withColumn(colName,sha1(concat_ws(\"~~\", *self.inputDf.columns)))\n",
                "    return self.inputDf\n",
                "\n",
                "# Convert Julian Date to Calendar Date  \n",
                "  def julian_to_calendar(self,subset):\n",
                "    julCol = (x for x in self.inputSchema if str(x.dataType)==\"IntegerType\" and x.name in subset)\n",
                "    for x in julCol:\n",
                "      self.inputDf = (self.inputDf.withColumn(x.name,col(x.name).cast(\"string\"))\n",
                "                                 .withColumn(x.name+\"_year\",\n",
                "                                             when((length(col(x.name))==5) & (substring(col(x.name),1,2) <=50),concat(lit('20'),substring(col(x.name),1,2)))\n",
                "                                             .when((length(col(x.name))==5) & (substring(col(x.name),1,2) >50),concat(lit('19'),substring(col(x.name),1,2)))\n",
                "                                             .when(length(col(x.name))==7,substring(col(x.name),1,4))\n",
                "                                             .otherwise(lit(0))\n",
                "                                            )\n",
                "                                 .withColumn(x.name+\"_days\",\n",
                "                                             when(length(col(x.name))==5,substring(col(x.name),3,3).cast(\"int\"))\n",
                "                                             .when(length(col(x.name))==7,substring(col(x.name),5,3).cast(\"int\"))\n",
                "                                             .otherwise(lit(0))\n",
                "                                            )\n",
                "                                 .withColumn(x.name+\"_ref_year\",concat(col(x.name+\"_year\"),lit(\"-01\"),lit(\"-01\")).cast(\"date\"))\n",
                "                                 .withColumn(x.name+\"_calendar\",expr(\"date_add(\" + x.name+\"_ref_year\"+\",\"+ x.name+\"_days)-1\"))\n",
                "                                 .drop(x.name, x.name+\"_year\",x.name+\"_days\",x.name+\"_ref_year\")\n",
                "                                 .withColumnRenamed(x.name+\"_calendar\",x.name)\n",
                "                                 \n",
                "                     )\n",
                "    return self.inputDf \n",
                "  \n",
                "# Convert Calendar Date to Julian Date \n",
                "  def calendar_to_julian(self, subset):\n",
                "    calCol = (x for x in self.inputSchema if ((str(x.dataType)==\"DateType\" or str(x.dataType)==\"TimestampType\") and x.name in subset))\n",
                "\n",
                "    for x in calCol:\n",
                "      self.inputDf = (self.inputDf.withColumn(x.name+\"_ref_year\", concat(year(col(x.name)).cast(\"string\"),lit(\"-01\"),lit(\"-01\")))\n",
                "                                  .withColumn(x.name+\"_datediff\", datediff(col(x.name),col(x.name+\"_ref_year\"))+1)\n",
                "                                  .withColumn(x.name+\"_julian\", concat(substring(year(col(x.name)).cast(\"string\"),3,2),col(x.name+\"_datediff\")).cast(\"int\"))\n",
                "                                  .drop(x.name,x.name+\"_ref_year\",x.name+\"_datediff\")\n",
                "                                  .withColumnRenamed(x.name+\"_julian\",x.name)\n",
                "                     )\n",
                "    return self.inputDf\n",
                "\n",
                "# Add a set of literal value columns to dataframe, pass as dictionary parameter  \n",
                "  def addLitCols(self,colDict):\n",
                "    for x in colDict.items():\n",
                "      self.inputDf = self.inputDf.withColumn(x[0],lit(x[1]))\n",
                "    return self.inputDf\n",
                "\n",
                "# Flattens a JSON/XMLN\n",
                "  def flattenNested(df):\n",
                "      ###########################################################################################################################  \n",
                "      # Function: flattenNested\n",
                "      # Returns a flattened version of XML, JSON\n",
                "      #\n",
                "      # Parameters:\n",
                "      # df = Dataframe loaded with the Nested XML or JSON structure\n",
                "      #\n",
                "      # Returns:\n",
                "      # The dataframe with the flattened version of the XML or JSON structure\n",
                "      ##########################################################################################################################      \n",
                "      structCols = []\n",
                "      sep = \"_\"\n",
                "\n",
                "      for colName, colType in df.dtypes:\n",
                "          if colType.startswith(\"struct\"):\n",
                "              structCols.append(colName)\n",
                "\n",
                "      arrayCols = []\n",
                "      for colName, colType in df.dtypes:\n",
                "          if colType.startswith(\"array\"):\n",
                "              arrayCols.append(colName)\n",
                "\n",
                "      if structCols:  \n",
                "          structElement = []\n",
                "          for colName, colType in df.dtypes:\n",
                "              if colType.startswith(\"struct\"):\n",
                "                  structElement.append(colName)\n",
                "          flattenCols = [fc for fc, _ in df.dtypes if fc not in structElement]\n",
                "          for nc in structElement:\n",
                "              for cc in df.select(f\"{nc}.*\").columns:\n",
                "                  flattenCols.append(F.col(f\"{nc}.{cc}\").alias(f\"{nc}{sep}{cc}\"))\n",
                "\n",
                "          df = df.select(flattenCols)          \n",
                "          return FlattenNested(df)\n",
                "\n",
                "      if arrayCols:\n",
                "          arrayElement = []\n",
                "          for colName, colType in df.dtypes:\n",
                "              if colType.startswith(\"array\"):\n",
                "                  arrayElement.append(colName)\n",
                "\n",
                "          explodeddf = df\n",
                "          for nc in arrayElement:\n",
                "              explodeddf = explodeddf.withColumn(nc, F.explode_outer(F.col(nc)))     \n",
                "\n",
                "          df = explodeddf \n",
                "          return FlattenNested(df) \n",
                "\n",
                "      return df      "
            ],
            "outputs": [],
            "execution_count": 2,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "id": "e1aa150d-36ae-4cc3-8b31-2b5bd05a6f36"
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "language_info": {
            "name": "python"
        },
        "widgets": {},
        "microsoft": {
            "language": "python",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            },
            "language_group": "synapse_pyspark"
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "notebook_environment": {},
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "description": "A collection of reusable Python classes that extends out of box PySpark capabilities",
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "enableDebugMode": false,
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        },
        "dependencies": {
            "lakehouse": {}
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}